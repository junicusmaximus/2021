{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_text.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWakCoFwAv2-"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pscOb6MIA1me",
        "outputId": "97178d80-cbbe-4f83-ae95-5eba550ecaa0"
      },
      "source": [
        "input_str = 'apple'\n",
        "label_str = 'pple!'\n",
        "char_vocab = sorted(list(set(input_str+label_str))) #set 중복 제거\n",
        "print(char_vocab)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['!', 'a', 'e', 'l', 'p']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWFx_1HNBH7b",
        "outputId": "438c981f-ae3c-4aeb-d021-f9051d68ad9e"
      },
      "source": [
        "vocab_size = len(char_vocab)\n",
        "print(f'문자 집합의 크기:{vocab_size}')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "문자 집합의 크기:5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7275557gDffw"
      },
      "source": [
        "input_size = vocab_size\n",
        "hidden_size = 5\n",
        "output_size = 5\n",
        "lr = 0.1"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGxcPv1fDwSs",
        "outputId": "8ad0dcdb-a467-4616-f75d-031050ff31f4"
      },
      "source": [
        "char_to_index = dict((c,i) for i,c in enumerate(char_vocab))\n",
        "print(char_to_index)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peOjJPWlD3Yt",
        "outputId": "cf084ae5-75ae-4b15-9d1f-1e1f8c643214"
      },
      "source": [
        "index_to_char = {} #나중에 예측 결과를 다시 문자 시퀀스로 보기 위해서 반대로도 만들어 놓음\n",
        "for key,value in char_to_index.items():\n",
        "  index_to_char[value] = key\n",
        "print(index_to_char)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wf5GSamMEUiV",
        "outputId": "e20b8f77-826d-4105-a60c-7f1ab74f74c5"
      },
      "source": [
        "x_data = [char_to_index[c] for c in input_str]\n",
        "y_data = [char_to_index[c] for c in label_str]\n",
        "print(x_data) #a,p,p,l,e\n",
        "print(y_data) #p,p,l,e,!"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 4, 4, 3, 2]\n",
            "[4, 4, 3, 2, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YEaBbdrFRZ0",
        "outputId": "d18e8fea-3ae0-446c-9a42-6cb61d5bcb9e"
      },
      "source": [
        "#파이토치의 nn.RNN()은 기본적으로 3차원 텐서를 입력 받음. 그렇기 떄문에 배치 차원을 추가해준다\n",
        "#배치 차원 추가\n",
        "#텐서 연산인 unsqueeze(0)를 통해 해결할 수도 있음\n",
        "\n",
        "x_data = [x_data]\n",
        "y_data = [y_data]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 4, 4, 3, 2]]\n",
            "[[4, 4, 3, 2, 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCNYXj4KHTmx",
        "outputId": "fc51dc5b-c0e9-464c-d064-04891e507fa6"
      },
      "source": [
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data] #총 행렬의 크기 5x5로 만들어라 \n",
        "print(x_one_hot)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[array([[0., 1., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1., 0.],\n",
            "       [0., 0., 1., 0., 0.]])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxQBKr0DKqCt",
        "outputId": "d6ba485a-4135-4395-e393-4cdb0db6e0f0"
      },
      "source": [
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)\n",
        "\n",
        "print(f'훈련 데이터의 크기:{X.shape}')\n",
        "print(f'레이블의 크기:{Y.shape}')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 데이터의 크기:torch.Size([1, 5, 5])\n",
            "레이블의 크기:torch.Size([1, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSYCqgT6LuIE"
      },
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self,input_size,hidden_size,output_size):\n",
        "    super(Net,self).__init__()\n",
        "    self.rnn = nn.RNN(input_size,hidden_size,batch_first=True) #RNN 셀 구현\n",
        "\n",
        "    self.fc = nn.Linear(hidden_size,output_size,bias=True) #출력층 구현\n",
        "\n",
        "  def forward(self,x):\n",
        "    x,_status = self.rnn(x)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKk9dmriMkPS"
      },
      "source": [
        "net = Net(input_size,hidden_size,output_size)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GrScTbWrNIui",
        "outputId": "17959156-1823-4b01-8eba-b02a86b918cb"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "oprimizer = optim.Adam(net.parameters(),lr)\n",
        "\n",
        "outputs = net(X)\n",
        "print(outputs)\n",
        "print('')\n",
        "print(outputs.shape) #배치 차원, 시점(timesteps), 출력의 크기 "
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[ 1.9191e-02, -4.2972e-01,  2.9179e-02, -1.6109e-01,  1.4142e-01],\n",
            "         [-1.2390e-02, -5.2069e-01, -6.0742e-01, -4.8536e-01, -8.3924e-02],\n",
            "         [-8.8123e-02, -5.3764e-01, -6.3862e-01, -4.6396e-01, -8.3717e-02],\n",
            "         [-8.0806e-02, -5.1205e-01, -6.1621e-01, -4.3224e-01,  3.1076e-04],\n",
            "         [-1.7154e-01, -5.4297e-01, -3.9047e-01, -2.1034e-01,  1.1845e-01]]],\n",
            "       grad_fn=<AddBackward0>)\n",
            "\n",
            "torch.Size([1, 5, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfvtUcrVP_MB",
        "outputId": "876af4c8-3e29-495e-96a3-85c13b430f44"
      },
      "source": [
        "print(outputs.view(-1,input_size).shape) #2차원 텐서로 변환 "
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pm_avgcbAdRb",
        "outputId": "421c9bad-304a-46f0-b347-b94a2ead4d93"
      },
      "source": [
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 5])\n",
            "torch.Size([5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXmoG1AvBRH2"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(),lr)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkH69G5DBaiv",
        "outputId": "e0a9b298-3b93-4f45-9887-e10c10edafb5"
      },
      "source": [
        "for i in range(10):\n",
        "  optimizer.zero_grad()\n",
        "  outputs = net(X)\n",
        "  loss = criterion(outputs.view(-1,input_size), Y.view(-1)) #view를 하는 이유 Batch차원 제거를 위해\n",
        "\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  #모델이 실제 어떻게 예측했는지를 확인하기 위한 코드\n",
        "  result = outputs.data.numpy().argmax(axis=2)\n",
        "  result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
        "  print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 loss:  1.6042366027832031 prediction:  [[4 0 4 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  p!ppp\n",
            "1 loss:  1.4144153594970703 prediction:  [[4 4 4 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppppp\n",
            "2 loss:  1.3271619081497192 prediction:  [[4 4 4 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppppp\n",
            "3 loss:  1.2430455684661865 prediction:  [[4 4 4 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppppp\n",
            "4 loss:  1.139857292175293 prediction:  [[4 4 4 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppppp\n",
            "5 loss:  1.0199990272521973 prediction:  [[4 4 4 0 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppp!!\n",
            "6 loss:  0.8814024925231934 prediction:  [[4 4 4 0 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppp!!\n",
            "7 loss:  0.7277898788452148 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "8 loss:  0.6384915113449097 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "9 loss:  0.48990267515182495 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnEVUbRHFE5k"
      },
      "source": [
        "## 더 많은 문장으로 했을때는?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3lLVqy8B55x"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim "
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFw4-K8LFD2q"
      },
      "source": [
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LNbk0w2Piwb",
        "outputId": "bfe1e54f-6177-418b-f912-92344a81e484"
      },
      "source": [
        "len(sentence)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "180"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFXHl7kmFDrf",
        "outputId": "bd364654-29df-4594-84bd-3af3941dea46"
      },
      "source": [
        "char_set = list(set(sentence)) #중복을 제거한 문자 집합\n",
        "char_dic = {c: i for i, c in enumerate(char_set)} #각 문자에 정수 인코딩\n",
        "print(char_dic)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'i': 0, 'a': 1, 'w': 2, 'r': 3, '.': 4, 'u': 5, 'y': 6, \"'\": 7, 'k': 8, ',': 9, 'g': 10, 'c': 11, 'n': 12, 'h': 13, 'b': 14, 's': 15, 'd': 16, 'o': 17, 'l': 18, 'e': 19, 'f': 20, 't': 21, 'p': 22, ' ': 23, 'm': 24}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmPv2DXQFbz1",
        "outputId": "78c0704c-90ef-4723-988b-dde016e9e3ee"
      },
      "source": [
        "dic_size = len(char_dic)\n",
        "print(f'문자 집합의 크기:{dic_size}')"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "문자 집합의 크기:25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O2rWPS4F5ms"
      },
      "source": [
        "hidden_size = dic_size\n",
        "sequence_length = 10\n",
        "lr = 0.1"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87g-5zqcGkFs",
        "outputId": "ac017b7c-eaa0-496a-9235-f6bedc923cf0"
      },
      "source": [
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0,len(sentence) - sequence_length):\n",
        "  x_str = sentence[i: i + sequence_length]\n",
        "  y_str = sentence[i+1: i + sequence_length + 1]\n",
        "  print(i, x_str, '->', y_str)\n",
        "\n",
        "  x_data.append([char_dic[c] for c in x_str]) # x str to index\n",
        "  y_data.append([char_dic[c] for c in y_str]) # y str to index"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 if you wan -> f you want\n",
            "1 f you want ->  you want \n",
            "2  you want  -> you want t\n",
            "3 you want t -> ou want to\n",
            "4 ou want to -> u want to \n",
            "5 u want to  ->  want to b\n",
            "6  want to b -> want to bu\n",
            "7 want to bu -> ant to bui\n",
            "8 ant to bui -> nt to buil\n",
            "9 nt to buil -> t to build\n",
            "10 t to build ->  to build \n",
            "11  to build  -> to build a\n",
            "12 to build a -> o build a \n",
            "13 o build a  ->  build a s\n",
            "14  build a s -> build a sh\n",
            "15 build a sh -> uild a shi\n",
            "16 uild a shi -> ild a ship\n",
            "17 ild a ship -> ld a ship,\n",
            "18 ld a ship, -> d a ship, \n",
            "19 d a ship,  ->  a ship, d\n",
            "20  a ship, d -> a ship, do\n",
            "21 a ship, do ->  ship, don\n",
            "22  ship, don -> ship, don'\n",
            "23 ship, don' -> hip, don't\n",
            "24 hip, don't -> ip, don't \n",
            "25 ip, don't  -> p, don't d\n",
            "26 p, don't d -> , don't dr\n",
            "27 , don't dr ->  don't dru\n",
            "28  don't dru -> don't drum\n",
            "29 don't drum -> on't drum \n",
            "30 on't drum  -> n't drum u\n",
            "31 n't drum u -> 't drum up\n",
            "32 't drum up -> t drum up \n",
            "33 t drum up  ->  drum up p\n",
            "34  drum up p -> drum up pe\n",
            "35 drum up pe -> rum up peo\n",
            "36 rum up peo -> um up peop\n",
            "37 um up peop -> m up peopl\n",
            "38 m up peopl ->  up people\n",
            "39  up people -> up people \n",
            "40 up people  -> p people t\n",
            "41 p people t ->  people to\n",
            "42  people to -> people tog\n",
            "43 people tog -> eople toge\n",
            "44 eople toge -> ople toget\n",
            "45 ople toget -> ple togeth\n",
            "46 ple togeth -> le togethe\n",
            "47 le togethe -> e together\n",
            "48 e together ->  together \n",
            "49  together  -> together t\n",
            "50 together t -> ogether to\n",
            "51 ogether to -> gether to \n",
            "52 gether to  -> ether to c\n",
            "53 ether to c -> ther to co\n",
            "54 ther to co -> her to col\n",
            "55 her to col -> er to coll\n",
            "56 er to coll -> r to colle\n",
            "57 r to colle ->  to collec\n",
            "58  to collec -> to collect\n",
            "59 to collect -> o collect \n",
            "60 o collect  ->  collect w\n",
            "61  collect w -> collect wo\n",
            "62 collect wo -> ollect woo\n",
            "63 ollect woo -> llect wood\n",
            "64 llect wood -> lect wood \n",
            "65 lect wood  -> ect wood a\n",
            "66 ect wood a -> ct wood an\n",
            "67 ct wood an -> t wood and\n",
            "68 t wood and ->  wood and \n",
            "69  wood and  -> wood and d\n",
            "70 wood and d -> ood and do\n",
            "71 ood and do -> od and don\n",
            "72 od and don -> d and don'\n",
            "73 d and don' ->  and don't\n",
            "74  and don't -> and don't \n",
            "75 and don't  -> nd don't a\n",
            "76 nd don't a -> d don't as\n",
            "77 d don't as ->  don't ass\n",
            "78  don't ass -> don't assi\n",
            "79 don't assi -> on't assig\n",
            "80 on't assig -> n't assign\n",
            "81 n't assign -> 't assign \n",
            "82 't assign  -> t assign t\n",
            "83 t assign t ->  assign th\n",
            "84  assign th -> assign the\n",
            "85 assign the -> ssign them\n",
            "86 ssign them -> sign them \n",
            "87 sign them  -> ign them t\n",
            "88 ign them t -> gn them ta\n",
            "89 gn them ta -> n them tas\n",
            "90 n them tas ->  them task\n",
            "91  them task -> them tasks\n",
            "92 them tasks -> hem tasks \n",
            "93 hem tasks  -> em tasks a\n",
            "94 em tasks a -> m tasks an\n",
            "95 m tasks an ->  tasks and\n",
            "96  tasks and -> tasks and \n",
            "97 tasks and  -> asks and w\n",
            "98 asks and w -> sks and wo\n",
            "99 sks and wo -> ks and wor\n",
            "100 ks and wor -> s and work\n",
            "101 s and work ->  and work,\n",
            "102  and work, -> and work, \n",
            "103 and work,  -> nd work, b\n",
            "104 nd work, b -> d work, bu\n",
            "105 d work, bu ->  work, but\n",
            "106  work, but -> work, but \n",
            "107 work, but  -> ork, but r\n",
            "108 ork, but r -> rk, but ra\n",
            "109 rk, but ra -> k, but rat\n",
            "110 k, but rat -> , but rath\n",
            "111 , but rath ->  but rathe\n",
            "112  but rathe -> but rather\n",
            "113 but rather -> ut rather \n",
            "114 ut rather  -> t rather t\n",
            "115 t rather t ->  rather te\n",
            "116  rather te -> rather tea\n",
            "117 rather tea -> ather teac\n",
            "118 ather teac -> ther teach\n",
            "119 ther teach -> her teach \n",
            "120 her teach  -> er teach t\n",
            "121 er teach t -> r teach th\n",
            "122 r teach th ->  teach the\n",
            "123  teach the -> teach them\n",
            "124 teach them -> each them \n",
            "125 each them  -> ach them t\n",
            "126 ach them t -> ch them to\n",
            "127 ch them to -> h them to \n",
            "128 h them to  ->  them to l\n",
            "129  them to l -> them to lo\n",
            "130 them to lo -> hem to lon\n",
            "131 hem to lon -> em to long\n",
            "132 em to long -> m to long \n",
            "133 m to long  ->  to long f\n",
            "134  to long f -> to long fo\n",
            "135 to long fo -> o long for\n",
            "136 o long for ->  long for \n",
            "137  long for  -> long for t\n",
            "138 long for t -> ong for th\n",
            "139 ong for th -> ng for the\n",
            "140 ng for the -> g for the \n",
            "141 g for the  ->  for the e\n",
            "142  for the e -> for the en\n",
            "143 for the en -> or the end\n",
            "144 or the end -> r the endl\n",
            "145 r the endl ->  the endle\n",
            "146  the endle -> the endles\n",
            "147 the endles -> he endless\n",
            "148 he endless -> e endless \n",
            "149 e endless  ->  endless i\n",
            "150  endless i -> endless im\n",
            "151 endless im -> ndless imm\n",
            "152 ndless imm -> dless imme\n",
            "153 dless imme -> less immen\n",
            "154 less immen -> ess immens\n",
            "155 ess immens -> ss immensi\n",
            "156 ss immensi -> s immensit\n",
            "157 s immensit ->  immensity\n",
            "158  immensity -> immensity \n",
            "159 immensity  -> mmensity o\n",
            "160 mmensity o -> mensity of\n",
            "161 mensity of -> ensity of \n",
            "162 ensity of  -> nsity of t\n",
            "163 nsity of t -> sity of th\n",
            "164 sity of th -> ity of the\n",
            "165 ity of the -> ty of the \n",
            "166 ty of the  -> y of the s\n",
            "167 y of the s ->  of the se\n",
            "168  of the se -> of the sea\n",
            "169 of the sea -> f the sea.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQJVuy9zHCBc",
        "outputId": "956c21d8-86e1-4da3-f342-fac092087772"
      },
      "source": [
        "print(x_data[0]) #if you wan에 해당\n",
        "print(y_data[0]) # f you want에 해당\n",
        "#한 칸씩 쉬프트로 된 시퀀스 "
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 20, 23, 6, 17, 5, 23, 2, 1, 12]\n",
            "[20, 23, 6, 17, 5, 23, 2, 1, 12, 21]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iudy3LC3HaJT"
      },
      "source": [
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data] #x 데이터는 원-핫 인코딩\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDBAt9HtH1VR",
        "outputId": "49e58640-e858-4067-f121-fdda63b122d4"
      },
      "source": [
        "print(f'훈련 데이터의 크기:{X.shape}')\n",
        "print(f'레이블의 크기:{Y.shape}')"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 데이터의 크기:torch.Size([170, 10, 25])\n",
            "레이블의 크기:torch.Size([170, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go6lmr6mIBk2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b9a52a4-dfaa-4f80-ad56-b8dbf775c6ee"
      },
      "source": [
        "print(X[0])"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 1., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 1., 0.],\n",
            "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzpVVwWVUj2S"
      },
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layers): # 현재 hidden_size는 dic_size와 같음.\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqLQVgpzW61-"
      },
      "source": [
        "net = Net(dic_size, hidden_size, 2) # 이번에는 층을 두 개 쌓습니다."
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLJu01YMW-2y"
      },
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t-vfyyhXAMZ",
        "outputId": "79ca98d2-1395-4b75-92e9-b8f8ef357249"
      },
      "source": [
        "outputs = net(X)\n",
        "print(outputs.shape) # 3차원 텐서"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([170, 10, 25])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOh7E0hZXDhR",
        "outputId": "e3ecac62-e72a-4297-bcf2-f8d224dba36d"
      },
      "source": [
        "print(outputs.view(-1, dic_size).shape) # 2차원 텐서로 변환."
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1700, 25])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGDmsAlVXE9j",
        "outputId": "4aad2f2e-61c8-4074-8115-eed0528ec71d"
      },
      "source": [
        "for i in range(30):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X) # (170, 10, 25) 크기를 가진 텐서를 매 에포크마다 모델의 입력으로 사용\n",
        "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # results의 텐서 크기는 (170, 10)\n",
        "    results = outputs.argmax(dim=2)\n",
        "    predict_str = \"\"\n",
        "    for j, result in enumerate(results):\n",
        "        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
        "            predict_str += ''.join([char_set[t] for t in result])\n",
        "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
        "            predict_str += char_set[result[-1]]\n",
        "\n",
        "    print(predict_str)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b,,,bn,,nbbn,b,,n,,,bnb,,nn,,nbb,,b,,nnbnnbnn,n,n,,b,n,,n,b,b,nb,,nnbb,bb,,,b,,,bb,,b,n,nnb,nnnnb,bn,nb,b,,,bbn,,nn,b,nb,n,b,n,n,b,,nnb,b,,bb,b,b,b,nn,,,bnn,,bnnnnbbnb,,,b,,,n,,n,\n",
            "                                                                                                                                                                                   \n",
            " bwo lbo  lbo lbmtl oolello lollle   lleo le  lbe lle  llleo lollll  lo  lbo  bo  leeo llb  lee to llllol lo  leoo ebo  lbo o lollo lleo le  leo  eo  lb  bl  lb lbd lle lllo   lll\n",
            "e to  to   to  o to to  e   to  te     to t   to to       to to to   to   o e to      e to to   to  e toe to  o to  o  o   to  o to   to  o t to  to  to  o  t    to to to toe to  \n",
            "t toa to nit   a io t a t   t a to ns tta tn  t  toa .  n taetn .ot  tneetooh tna t tnni.mttae sta ns t a tot t tns w t t   a te t tn ta th w t a tnanw'. a teana tnitottn .otnt t \n",
            "totoa tand tonthsoa todotoa tod'toaii'tan toam tototo tonitoniplaoio ton'laos ton ttoniito tae'ltosmittns ton ttani'to mso  anihiao ' tonihla tonitoe ipait toton thito hiitaemi'tt\n",
            "to on to d th to ta to o md tod toap t tn tomm a th p aor th th t ro th t dos dor  oamsito th m th m itod dh ta to tth o p tn th ta m th th t th  to m od doto on o  to th th m t  \n",
            "co on tn d th to  d dodo pd dod  odod  to trm  d torm o m to th t ro thr  dod dor  odod th toem to m  dod dor d dod do h p to th taem th tort tor toep od d  otod os do od toep o  \n",
            "co on to g tortoe d dod eod dod  od r  to tarm o tor  oem torto  trm tor  ued dor  odod  o toer toem  dod dor t dod docoep to ch toer to lhr  wor toe  od d m tom od to on toe  or \n",
            "lo on tong woncoe d d r eoe aod   a d  uo tanm u tor  oem tonlh lerd wong aod aon   aoe to toem toems uod aor t wod uocoer tonch them to lheg tonltoe tod u e tom os to on toe  om \n",
            "l ion tong wo ths d d u ir  a d't a d  tn te m e tor toem to th terd wong and aodkt ane m ltoem toems and aon t wod a cher to ch them to thng tonltoe tos ane tom oslt lon toe  emt\n",
            "r ion tont to to le dnd ir  dnd t a d  tm te   e tog toem to th terd wonl and andkt ans tsltoem toeps dnd ao  t wot anther to ch them to teng to ltoe to  ent to to lt ion toemeemt\n",
            "r iof tong to tosle dnt ir  tod t a d  us pens e teg them to th teid won  and aod t ans ts them toep  dnd aon t wot anther to ch them to teng ton toe tos ent to  n  toiof toemte t\n",
            "r ior'tand wo losle dst ips tog't and  as pens e tog  hem to to lend wor  and aog't ans tm them toems dnd aor t wot a ther to ch them to leng wor toemtod ens iomens th of themtemt\n",
            "r bor'tant wonlosle dsteips dog't aou  ah psns e the them to lo lend wor  and don't ansetm them tasms dnd aor t woc a ther to ch them ta long wor themeoi ens iomons tm of themteme\n",
            "rheor'lont bortosld dnteip, don't aos' ap peos e tog them to lo lecd wor' dnd don't ans tm them tosms dnd dor t bos dosher toach them to long wor'toemeed enseiomons th on themeeie\n",
            "r eor'lant tontodld anseip, don't aou  up peos e tog them to lo lect wor' dnd don't ansetm them tosms dnd dorkt bos fosher toach them to long wor'toemeodle s iomens ty on themeeme\n",
            "r eorttant to todld a seip, don't aout ap peosle tog them to lo lecd work and dor't assecn toem toskd dnd dorkt but fother toach them to long wor themeodle s ipmenscty on themep c\n",
            "c eorttant to todld a ship, don't aout up peosle tog ther thaco lecd work and don't assegn toem tosks dnd dorkt but fother toach them to cong wor themendle s ipmensity of themenrc\n",
            "g eorktand to budld dnship, don't aoup up peop e tog ther to lo lect word and don't ansign them tosks and work, but fother toach them to long wor the endlens ipmensity of themensc\n",
            "g worttant to build a ship, don't aoum up peop e tog ther to lo lect ward and won't assign them tosks and work, but father toach the  to long wor the endlecs ipmensity of the eenc\n",
            "t bor wont to build a s ip, don't aoum up people together to bo lect ward and don't assign them tosks and dork, but a ther toach the  to long wor the endless ipmensity of the senc\n",
            "t boo wont tolbuild a ship, don't arum up peop e together to bollect ward and don't assign them tosks and dork, but rother toach them to long for themendless immensity of the senc\n",
            "tyboo wont to build a ship, don't arum up people together to collect word dnd don't assign them tosks and dork, but rother toach them to longhfor themendless immensity of themeerc\n",
            "tyboo want to build a ship, don't arum up people together to collect word dnd won't assign them tasks and work, but rother toach them to long for the endless immensity of themseac\n",
            "tybou want to build a ship, don't arum up people together to collect wood dnd won't assign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
            "tybou want to build a ship, don't arum up people together th collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the seac\n",
            "tybou want to build a ship, don't arum up people together te collect wood and don't assign them tasks and dork, but rather teach them ta long for the endless immensity of the shac\n",
            "tybou want to build a ship, don't arum up people together to collect wood and don't dssign them tasks and work, but rather teach them ta long for the endless immensity of the seac\n",
            "tyyou want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the seac\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8HwXh8VXXl6"
      },
      "source": [
        "## Embedding 사용!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MilZiJRIXY-Y"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHpegg6oZKyk",
        "outputId": "1f556b0a-a79c-43c5-e998-c70c7d5b6e2a"
      },
      "source": [
        "sentence = \"Repeat is the best medicine for memory\".split()\n",
        "vocab = list(set(sentence))\n",
        "print(vocab)\n",
        "#우리가 만들 RNN은 \"Repeat is the best medicine for\"을 입력 받으면 \"is the best medicine for memory\"를 출력하는 RNN"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['for', 'Repeat', 'best', 'the', 'is', 'memory', 'medicine']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBkUSgsAZVS8",
        "outputId": "155e7505-fa93-4be7-f786-d8f58917ceb8"
      },
      "source": [
        "word2index = {tkn:i for i,tkn in enumerate(vocab,1)} #단어에 고유한 정수 부여\n",
        "word2index['<unk>']=0\n",
        "print(word2index)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'for': 1, 'Repeat': 2, 'best': 3, 'the': 4, 'is': 5, 'memory': 6, 'medicine': 7, '<unk>': 0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKi_bE3v_d63",
        "outputId": "d3ac6c05-796a-4467-b9a9-25de00c7eb4d"
      },
      "source": [
        "index2word = {v: k for k, v in word2index.items()}\n",
        "print(index2word)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{1: 'for', 2: 'Repeat', 3: 'best', 4: 'the', 5: 'is', 6: 'memory', 7: 'medicine', 0: '<unk>'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-fhlcwHbP2n"
      },
      "source": [
        "#우리가 사용할 word2index단어장\n",
        "def build_data(sentence,word2index):\n",
        "  encoded = [word2index[token] for token in sentence]\n",
        "  input_seq, label_seq = encoded[:-1],encoded[1:]\n",
        "  input_seq = torch.LongTensor(input_seq).unsqueeze(0)\n",
        "  label_seq = torch.LongTensor(label_seq).unsqueeze(0)\n",
        "  return input_seq,label_seq"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PENpv29NlA-z",
        "outputId": "96bc463b-c083-4982-dcbc-78bfd8cbaf15"
      },
      "source": [
        "X,Y = build_data(sentence,word2index)\n",
        "print(X)\n",
        "print(Y)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2, 5, 4, 3, 7, 1]])\n",
            "tensor([[5, 4, 3, 7, 1, 6]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpMgwFtflIo5"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, vocab_size, input_size, hidden_size, batch_first=True):\n",
        "        super(Net, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, # 워드 임베딩\n",
        "                                            embedding_dim=input_size)\n",
        "        self.rnn_layer = nn.RNN(input_size, hidden_size, \n",
        "                                batch_first=batch_first)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size) \n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. 임베딩 층\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이) => (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
        "        output = self.embedding_layer(x)\n",
        "        # 2. RNN 층\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
        "        # => output (배치 크기, 시퀀스 길이, 은닉층 크기), hidden (1, 배치 크기, 은닉층 크기)\n",
        "        output, hidden = self.rnn_layer(output)\n",
        "        # 3. 최종 출력층\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 은닉층 크기) => (배치 크기, 시퀀스 길이, 단어장 크기)\n",
        "        output = self.linear(output)\n",
        "        # 4. view를 통해서 배치 차원 제거\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 단어장 크기) => (배치 크기*시퀀스 길이, 단어장 크기)\n",
        "        return output.view(-1, output.size(2))"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vf7R8LM25pPe"
      },
      "source": [
        "vocab_size = len(word2index) \n",
        "input_size = 5      \n",
        "hidden_size = 20   "
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGso6pNK9SGm"
      },
      "source": [
        "model = Net(vocab_size, input_size, hidden_size, batch_first=True)\n",
        "loss_function = nn.CrossEntropyLoss() \n",
        "optimizer = optim.Adam(params=model.parameters())"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpkAsxXf9HEk",
        "outputId": "63e811f0-1662-43db-ffd6-bbf9e76d83bc"
      },
      "source": [
        "#(시퀀스의 길이, 은닉층의 크기)\n",
        "output = model(X)\n",
        "print(output)\n",
        "print('')\n",
        "print(output.shape)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.2609,  0.2517, -0.4234,  0.1609,  0.2980, -0.6366,  0.0351, -0.1422],\n",
            "        [ 0.0825,  0.1859, -0.1430, -0.2159,  0.4267, -0.5054,  0.1280, -0.1824],\n",
            "        [ 0.4297,  0.1642, -0.1339,  0.0602,  0.0755, -0.3480,  0.0546, -0.1096],\n",
            "        [ 0.2407,  0.2043, -0.0137, -0.1367,  0.3346, -0.2834,  0.0984, -0.1080],\n",
            "        [ 0.3557,  0.1780,  0.1426,  0.0720,  0.1709, -0.2613, -0.0570, -0.2877],\n",
            "        [ 0.1746,  0.1438,  0.0553, -0.2467,  0.1375,  0.2544, -0.1970, -0.2312]],\n",
            "       grad_fn=<ViewBackward>)\n",
            "\n",
            "torch.Size([6, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVaaBqqt9PW_"
      },
      "source": [
        "decode = lambda y: [index2word.get(x) for x in y]"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LKFzpaE_McQ",
        "outputId": "24e87472-a143-475f-c8df-2b19aa1b7eae"
      },
      "source": [
        "for step in range(201):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X)\n",
        "    loss = loss_function(output, Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # 기록\n",
        "    if step % 40 == 0:\n",
        "        print(\"[{:02d}/201] {:.4f} \".format(step+1, loss))\n",
        "        pred = output.softmax(-1).argmax(-1).tolist()\n",
        "        print(\" \".join([\"Repeat\"] + decode(pred)))\n",
        "        print()"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[01/201] 1.9008 \n",
            "Repeat the the best the for the\n",
            "\n",
            "[41/201] 1.3098 \n",
            "Repeat the the best medicine for memory\n",
            "\n",
            "[81/201] 0.6863 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[121/201] 0.3627 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[161/201] 0.1997 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[201/201] 0.1181 \n",
            "Repeat is the best medicine for memory\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52XoIHY0CgSk"
      },
      "source": [
        "Reference: https://wikidocs.net/64765 를 참조하였습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huGNC3xACiXR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}